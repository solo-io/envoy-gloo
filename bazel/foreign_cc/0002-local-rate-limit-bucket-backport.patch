diff --git a/changelogs/current.yaml b/changelogs/current.yaml
index 9ecf0d6e48..4ba2097db3 100644
diff --git a/source/common/common/token_bucket_impl.cc b/source/common/common/token_bucket_impl.cc
index f813d426d0..b3d4f10e78 100644
--- source/common/common/token_bucket_impl.cc
+++ source/common/common/token_bucket_impl.cc
@@ -1,5 +1,6 @@
 #include "source/common/common/token_bucket_impl.h"
 
+#include <atomic>
 #include <chrono>
 
 namespace Envoy {
@@ -7,6 +8,7 @@ namespace Envoy {
 namespace {
 // The minimal fill rate will be one second every year.
 constexpr double kMinFillRate = 1.0 / (365 * 24 * 60 * 60);
+
 } // namespace
 
 TokenBucketImpl::TokenBucketImpl(uint64_t max_tokens, TimeSource& time_source, double fill_rate)
@@ -56,4 +58,44 @@ void TokenBucketImpl::maybeReset(uint64_t num_tokens) {
   last_fill_ = time_source_.monotonicTime();
 }
 
+AtomicTokenBucketImpl::AtomicTokenBucketImpl(uint64_t max_tokens, TimeSource& time_source,
+                                             double fill_rate, bool init_fill)
+    : max_tokens_(max_tokens), fill_rate_(std::max(std::abs(fill_rate), kMinFillRate)),
+      time_source_(time_source) {
+  auto time_in_seconds = timeNowInSeconds();
+  if (init_fill) {
+    time_in_seconds -= max_tokens_ / fill_rate_;
+  }
+  time_in_seconds_.store(time_in_seconds, std::memory_order_relaxed);
+}
+
+bool AtomicTokenBucketImpl::consume() {
+  constexpr auto consumed_cb = [](double total_tokens) -> double {
+    return total_tokens >= 1 ? 1 : 0;
+  };
+  return consume(consumed_cb) == 1;
+}
+
+uint64_t AtomicTokenBucketImpl::consume(uint64_t tokens, bool allow_partial) {
+  const auto consumed_cb = [tokens, allow_partial](double total_tokens) {
+    const auto consumed = static_cast<double>(tokens);
+    if (total_tokens >= consumed) {
+      return consumed; // There are enough tokens to consume.
+    }
+    // If allow_partial is true, consume all available tokens.
+    return allow_partial ? std::max<double>(0, std::floor(total_tokens)) : 0;
+  };
+  return static_cast<uint64_t>(consume(consumed_cb));
+}
+
+double AtomicTokenBucketImpl::remainingTokens() const {
+  const double time_now = timeNowInSeconds();
+  const double time_old = time_in_seconds_.load(std::memory_order_relaxed);
+  return std::min(max_tokens_, (time_now - time_old) * fill_rate_);
+}
+
+double AtomicTokenBucketImpl::timeNowInSeconds() const {
+  return std::chrono::duration<double>(time_source_.monotonicTime().time_since_epoch()).count();
+}
+
 } // namespace Envoy
diff --git a/source/common/common/token_bucket_impl.h b/source/common/common/token_bucket_impl.h
index 96ac238e37..673f0a74e2 100644
--- source/common/common/token_bucket_impl.h
+++ source/common/common/token_bucket_impl.h
@@ -35,4 +35,92 @@ private:
   TimeSource& time_source_;
 };
 
+/**
+ * Atomic token bucket. This class is thread-safe.
+ */
+class AtomicTokenBucketImpl {
+public:
+  /**
+   * @param max_tokens supplies the maximum number of tokens in the bucket.
+   * @param time_source supplies the time source.
+   * @param fill_rate supplies the number of tokens that will return to the bucket on each second.
+   * The default is 1.
+   * @param init_fill supplies whether the bucket should be initialized with max_tokens.
+   */
+  explicit AtomicTokenBucketImpl(uint64_t max_tokens, TimeSource& time_source,
+                                 double fill_rate = 1.0, bool init_fill = true);
+
+  // This reference https://github.com/facebook/folly/blob/main/folly/TokenBucket.h.
+  template <class GetConsumedTokens> double consume(const GetConsumedTokens& cb) {
+    const double time_now = timeNowInSeconds();
+
+    double time_old = time_in_seconds_.load(std::memory_order_relaxed);
+    double time_new{};
+    double consumed{};
+    do {
+      const double total_tokens = std::min(max_tokens_, (time_now - time_old) * fill_rate_);
+      if (consumed = cb(total_tokens); consumed == 0) {
+        return 0;
+      }
+
+      // There are two special cases that should rarely happen in practice but we will not
+      // prevent them in this common template method:
+      // The consumed is negative. It means the token is added back to the bucket.
+      // The consumed is larger than total_tokens. It means the bucket is overflowed and future
+      // tokens are consumed.
+
+      // Move the time_in_seconds_ forward by the number of tokens consumed.
+      const double total_tokens_new = total_tokens - consumed;
+      time_new = time_now - (total_tokens_new / fill_rate_);
+    } while (
+        !time_in_seconds_.compare_exchange_weak(time_old, time_new, std::memory_order_relaxed));
+
+    return consumed;
+  }
+
+  /**
+   * Consumes one tokens from the bucket.
+   * @return true if the token is consumed, false otherwise.
+   */
+  bool consume();
+
+  /**
+   * Consumes multiple tokens from the bucket.
+   * @param tokens the number of tokens to consume.
+   * @param allow_partial whether to allow partial consumption.
+   * @return the number of tokens consumed.
+   */
+  uint64_t consume(uint64_t tokens, bool allow_partial);
+
+  /**
+   * Get the maximum number of tokens in the bucket. The actual maximum number of tokens in the
+   * bucket may be changed with the factor.
+   * @return the maximum number of tokens in the bucket.
+   */
+  double maxTokens() const { return max_tokens_; }
+
+  /**
+   * Get the fill rate of the bucket. This is a constant for the lifetime of the bucket. But note
+   * the actual used fill rate will multiply the dynamic factor.
+   * @return the fill rate of the bucket.
+   */
+  double fillRate() const { return fill_rate_; }
+
+  /**
+   * Get the remaining number of tokens in the bucket. This is a snapshot and may change after the
+   * call.
+   * @return the remaining number of tokens in the bucket.
+   */
+  double remainingTokens() const;
+
+private:
+  double timeNowInSeconds() const;
+
+  const double max_tokens_;
+  const double fill_rate_;
+
+  std::atomic<double> time_in_seconds_{};
+  TimeSource& time_source_;
+};
+
 } // namespace Envoy
diff --git a/source/common/runtime/runtime_features.cc b/source/common/runtime/runtime_features.cc
index 260a08820c..f22d9cf58b 100644
--- source/common/runtime/runtime_features.cc
+++ source/common/runtime/runtime_features.cc
@@ -73,6 +73,7 @@ RUNTIME_GUARD(envoy_reloadable_features_lowercase_scheme);
 RUNTIME_GUARD(envoy_reloadable_features_no_downgrade_to_canonical_name);
 RUNTIME_GUARD(envoy_reloadable_features_no_extension_lookup_by_name);
 RUNTIME_GUARD(envoy_reloadable_features_no_full_scan_certs_on_sni_mismatch);
+// RUNTIME_GUARD(envoy_reloadable_features_no_timer_based_rate_limit_token_bucket);
 RUNTIME_GUARD(envoy_reloadable_features_normalize_host_for_preresolve_dfp_dns);
 RUNTIME_GUARD(envoy_reloadable_features_oauth_make_token_cookie_httponly);
 RUNTIME_GUARD(envoy_reloadable_features_oauth_use_standard_max_age_value);
@@ -113,6 +114,9 @@ RUNTIME_GUARD(envoy_restart_features_use_fast_protobuf_hash);
 
 // Begin false flags. Most of them should come with a TODO to flip true.
 
+//backporting this so TODO: Remove all this forked code before adding to 1.31
+FALSE_RUNTIME_GUARD(envoy_reloadable_features_no_timer_based_rate_limit_token_bucket);
+
 // TODO(birenroy) Flip this to true after resolving issues.
 // Ignore the automated "remove this flag" issue: we should keep this for 1 year.
 FALSE_RUNTIME_GUARD(envoy_reloadable_features_http2_use_oghttp2);
diff --git a/source/extensions/filters/common/local_ratelimit/BUILD b/source/extensions/filters/common/local_ratelimit/BUILD
index 2e6af5b6da..5cea645b04 100644
--- source/extensions/filters/common/local_ratelimit/BUILD
+++ source/extensions/filters/common/local_ratelimit/BUILD
@@ -17,6 +17,7 @@ envoy_cc_library(
         "//envoy/event:timer_interface",
         "//envoy/ratelimit:ratelimit_interface",
         "//source/common/common:thread_synchronizer_lib",
+        "//source/common/common:token_bucket_impl_lib",
         "//source/common/protobuf:utility_lib",
         "@envoy_api//envoy/extensions/common/ratelimit/v3:pkg_cc_proto",
     ],
diff --git a/source/extensions/filters/common/local_ratelimit/local_ratelimit_impl.cc b/source/extensions/filters/common/local_ratelimit/local_ratelimit_impl.cc
index 603a61eca0..7b6c8a5aef 100644
--- source/extensions/filters/common/local_ratelimit/local_ratelimit_impl.cc
+++ source/extensions/filters/common/local_ratelimit/local_ratelimit_impl.cc
@@ -13,6 +13,159 @@ namespace Filters {
 namespace Common {
 namespace LocalRateLimit {
 
+SINGLETON_MANAGER_REGISTRATION(local_ratelimit_share_provider_manager);
+
+class DefaultEvenShareMonitor : public ShareProviderManager::ShareMonitor {
+public:
+  double getTokensShareFactor() const override { return share_factor_.load(); }
+  double onLocalClusterUpdate(const Upstream::Cluster& cluster) override {
+    ASSERT_IS_MAIN_OR_TEST_THREAD();
+    const auto num = cluster.info()->endpointStats().membership_total_.value();
+    const double new_share_factor = num == 0 ? 1.0 : 1.0 / num;
+    share_factor_.store(new_share_factor);
+    return new_share_factor;
+  }
+
+private:
+  std::atomic<double> share_factor_{1.0};
+};
+
+ShareProviderManager::ShareProviderManager(Event::Dispatcher& main_dispatcher,
+                                           const Upstream::Cluster& cluster)
+    : main_dispatcher_(main_dispatcher), cluster_(cluster) {
+  // It's safe to capture the local cluster reference here because the local cluster is
+  // guaranteed to be static cluster and should never be removed.
+  handle_ = cluster_.prioritySet().addMemberUpdateCb([this](const auto&, const auto&) {
+    share_monitor_->onLocalClusterUpdate(cluster_);
+    return absl::OkStatus();
+  });
+  share_monitor_ = std::make_shared<DefaultEvenShareMonitor>();
+  share_monitor_->onLocalClusterUpdate(cluster_);
+}
+
+ShareProviderManager::~ShareProviderManager() {
+  // Ensure the callback is unregistered on the main dispatcher thread.
+  main_dispatcher_.post([h = std::move(handle_)]() {});
+}
+
+ShareProviderSharedPtr
+ShareProviderManager::getShareProvider(const ProtoLocalClusterRateLimit&) const {
+  // TODO(wbpcode): we may want to support custom share provider in the future based on the
+  // configuration.
+  return share_monitor_;
+}
+
+ShareProviderManagerSharedPtr ShareProviderManager::singleton(Event::Dispatcher& dispatcher,
+                                                              Upstream::ClusterManager& cm,
+                                                              Singleton::Manager& manager) {
+  return manager.getTyped<ShareProviderManager>(
+      SINGLETON_MANAGER_REGISTERED_NAME(local_ratelimit_share_provider_manager),
+      [&dispatcher, &cm]() -> Singleton::InstanceSharedPtr {
+        const auto& local_cluster_name = cm.localClusterName();
+        if (!local_cluster_name.has_value()) {
+          return nullptr;
+        }
+        auto cluster = cm.clusters().getCluster(local_cluster_name.value());
+        if (!cluster.has_value()) {
+          return nullptr;
+        }
+        return ShareProviderManagerSharedPtr{
+            new ShareProviderManager(dispatcher, cluster.value().get())};
+      });
+}
+
+TimerTokenBucket::TimerTokenBucket(uint32_t max_tokens, uint32_t tokens_per_fill,
+                                   std::chrono::milliseconds fill_interval, uint64_t multiplier,
+                                   LocalRateLimiterImpl& parent)
+    : multiplier_(multiplier), parent_(parent), max_tokens_(max_tokens),
+      tokens_per_fill_(tokens_per_fill), fill_interval_(fill_interval),
+      // Calculate the fill rate in tokens per second.
+      fill_rate_(tokens_per_fill /
+                 std::chrono::duration_cast<std::chrono::duration<double>>(fill_interval).count()) {
+  tokens_ = max_tokens;
+  fill_time_ = parent_.time_source_.monotonicTime();
+}
+
+absl::optional<int64_t> TimerTokenBucket::remainingFillInterval() const {
+  using namespace std::literals;
+
+  const auto time_after_last_fill = std::chrono::duration_cast<std::chrono::milliseconds>(
+      parent_.time_source_.monotonicTime() - fill_time_.load());
+
+  // Note that the fill timer may be delayed because other tasks are running on the main thread.
+  // So it's possible that the time_after_last_fill is greater than fill_interval_.
+  if (time_after_last_fill >= fill_interval_) {
+    return {};
+  }
+
+  return absl::ToInt64Seconds(absl::FromChrono(fill_interval_) -
+                              absl::Seconds((time_after_last_fill) / 1s));
+}
+
+bool TimerTokenBucket::consume(double) {
+  // Relaxed consistency is used for all operations because we don't care about ordering, just the
+  // final atomic correctness.
+  uint32_t expected_tokens = tokens_.load(std::memory_order_relaxed);
+  do {
+    // expected_tokens is either initialized above or reloaded during the CAS failure below.
+    if (expected_tokens == 0) {
+      return false;
+    }
+
+    // Testing hook.
+    parent_.synchronizer_.syncPoint("allowed_pre_cas");
+
+    // Loop while the weak CAS fails trying to subtract 1 from expected.
+  } while (!tokens_.compare_exchange_weak(expected_tokens, expected_tokens - 1,
+                                          std::memory_order_relaxed));
+
+  // We successfully decremented the counter by 1.
+  return true;
+}
+
+void TimerTokenBucket::onFillTimer(uint64_t refill_counter, double factor) {
+  // Descriptors are refilled every Nth timer hit where N is the ratio of the
+  // descriptor refill interval over the global refill interval. For example,
+  // if the descriptor refill interval is 150ms and the global refill
+  // interval is 50ms, this descriptor is refilled every 3rd call.
+  if (refill_counter % multiplier_ != 0) {
+    return;
+  }
+
+  const uint32_t tokens_per_fill = std::ceil(tokens_per_fill_ * factor);
+
+  // Relaxed consistency is used for all operations because we don't care about ordering, just the
+  // final atomic correctness.
+  uint32_t expected_tokens = tokens_.load(std::memory_order_relaxed);
+  uint32_t new_tokens_value{};
+  do {
+    // expected_tokens is either initialized above or reloaded during the CAS failure below.
+    new_tokens_value = std::min(max_tokens_, expected_tokens + tokens_per_fill);
+
+    // Testing hook.
+    parent_.synchronizer_.syncPoint("on_fill_timer_pre_cas");
+
+    // Loop while the weak CAS fails trying to update the tokens value.
+  } while (
+      !tokens_.compare_exchange_weak(expected_tokens, new_tokens_value, std::memory_order_relaxed));
+
+  // Update fill time at last.
+  fill_time_ = parent_.time_source_.monotonicTime();
+}
+
+AtomicTokenBucket::AtomicTokenBucket(uint32_t max_tokens, uint32_t tokens_per_fill,
+                                     std::chrono::milliseconds fill_interval,
+                                     TimeSource& time_source)
+    : token_bucket_(max_tokens, time_source,
+                    // Calculate the fill rate in tokens per second.
+                    tokens_per_fill / std::chrono::duration<double>(fill_interval).count()) {}
+
+bool AtomicTokenBucket::consume(double factor) {
+  ASSERT(!(factor <= 0.0 || factor > 1.0));
+  auto cb = [tokens = 1.0 / factor](double total) { return total < tokens ? 0.0 : tokens; };
+  return token_bucket_.consume(cb) != 0.0;
+}
+
 LocalRateLimiterImpl::LocalRateLimiterImpl(
     const std::chrono::milliseconds fill_interval, const uint32_t max_tokens,
     const uint32_t tokens_per_fill, Event::Dispatcher& dispatcher,
@@ -22,20 +175,25 @@ LocalRateLimiterImpl::LocalRateLimiterImpl(
     : fill_timer_(fill_interval > std::chrono::milliseconds(0)
                       ? dispatcher.createTimer([this] { onFillTimer(); })
                       : nullptr),
-      time_source_(dispatcher.timeSource()),
-      always_consume_default_token_bucket_(always_consume_default_token_bucket) {
+      time_source_(dispatcher.timeSource()), share_provider_(std::move(shared_provider)),
+      always_consume_default_token_bucket_(always_consume_default_token_bucket),
+      no_timer_based_rate_limit_token_bucket_(Runtime::runtimeFeatureEnabled(
+          "envoy.reloadable_features.no_timer_based_rate_limit_token_bucket")) {
   if (fill_timer_ && fill_interval < std::chrono::milliseconds(50)) {
     throw EnvoyException("local rate limit token bucket fill timer must be >= 50ms");
   }
 
-  token_bucket_.max_tokens_ = max_tokens;
-  token_bucket_.tokens_per_fill_ = tokens_per_fill;
-  token_bucket_.fill_interval_ = absl::FromChrono(fill_interval);
-  tokens_.tokens_ = max_tokens;
-  tokens_.fill_time_ = time_source_.monotonicTime();
+  if (no_timer_based_rate_limit_token_bucket_) {
+    default_token_bucket_ = std::make_shared<AtomicTokenBucket>(max_tokens, tokens_per_fill,
+                                                                fill_interval, time_source_);
+  } else {
+    default_token_bucket_ =
+        std::make_shared<TimerTokenBucket>(max_tokens, tokens_per_fill, fill_interval, 1, *this);
+  }
 
-  if (fill_timer_) {
-    fill_timer_->enableTimer(fill_interval);
+  if (fill_timer_ && default_token_bucket_->fillInterval().count() > 0 &&
+      !no_timer_based_rate_limit_token_bucket_) {
+    fill_timer_->enableTimer(default_token_bucket_->fillInterval());
   }
 
   for (const auto& descriptor : descriptors) {
@@ -59,10 +217,16 @@ LocalRateLimiterImpl::LocalRateLimiterImpl(
         PROTOBUF_GET_WRAPPED_OR_DEFAULT(descriptor.token_bucket(), tokens_per_fill, 1);
     new_descriptor.token_bucket_ = per_descriptor_token_bucket;
 
-    auto token_state = std::make_shared<TokenState>();
-    token_state->tokens_ = per_descriptor_token_bucket.max_tokens_;
-    token_state->fill_time_ = time_source_.monotonicTime();
-    new_descriptor.token_state_ = token_state;
+    RateLimitTokenBucketSharedPtr per_descriptor_token_bucket;
+    if (no_timer_based_rate_limit_token_bucket_) {
+      per_descriptor_token_bucket = std::make_shared<AtomicTokenBucket>(
+          per_descriptor_max_tokens, per_descriptor_tokens_per_fill, per_descriptor_fill_interval,
+          time_source_);
+    } else {
+      per_descriptor_token_bucket = std::make_shared<TimerTokenBucket>(
+          per_descriptor_max_tokens, per_descriptor_tokens_per_fill, per_descriptor_fill_interval,
+          per_descriptor_multiplier, *this);
+    }
 
     auto result = descriptors_.emplace(new_descriptor);
     if (!result.second) {
@@ -193,9 +357,13 @@ bool LocalRateLimiterImpl::requestAllowed(
     }
   }
 
-  if (!matched_descriptor || always_consume_default_token_bucket_) {
-    // Since global tokens are not sorted, it should be larger than other descriptors.
-    return requestAllowedHelper(tokens_);
+  if (matched_descriptors.size() > 1) {
+    // Sort the matched descriptors by token bucket fill rate to ensure the descriptor with the
+    // smallest fill rate is consumed first.
+    std::sort(matched_descriptors.begin(), matched_descriptors.end(),
+              [](const RateLimitTokenBucket* lhs, const RateLimitTokenBucket* rhs) {
+                return lhs->fillRate() < rhs->fillRate();
+              });
   }
   return true;
 }
diff --git a/source/extensions/filters/common/local_ratelimit/local_ratelimit_impl.h b/source/extensions/filters/common/local_ratelimit/local_ratelimit_impl.h
index c0cc182a49..26fa82e351 100644
--- source/extensions/filters/common/local_ratelimit/local_ratelimit_impl.h
+++ source/extensions/filters/common/local_ratelimit/local_ratelimit_impl.h
@@ -8,6 +8,7 @@
 #include "envoy/ratelimit/ratelimit.h"
 
 #include "source/common/common/thread_synchronizer.h"
+#include "source/common/common/token_bucket_impl.h"
 #include "source/common/protobuf/protobuf.h"
 
 namespace Envoy {
@@ -16,6 +17,115 @@ namespace Filters {
 namespace Common {
 namespace LocalRateLimit {
 
+using ProtoLocalClusterRateLimit = envoy::extensions::common::ratelimit::v3::LocalClusterRateLimit;
+
+class ShareProvider {
+public:
+  virtual ~ShareProvider() = default;
+  // The share of the tokens. This method should be thread-safe.
+  virtual double getTokensShareFactor() const PURE;
+};
+using ShareProviderSharedPtr = std::shared_ptr<ShareProvider>;
+
+class ShareProviderManager;
+using ShareProviderManagerSharedPtr = std::shared_ptr<ShareProviderManager>;
+
+class ShareProviderManager : public Singleton::Instance {
+public:
+  ShareProviderSharedPtr getShareProvider(const ProtoLocalClusterRateLimit& config) const;
+  ~ShareProviderManager() override;
+
+  static ShareProviderManagerSharedPtr singleton(Event::Dispatcher& dispatcher,
+                                                 Upstream::ClusterManager& cm,
+                                                 Singleton::Manager& manager);
+
+  class ShareMonitor : public ShareProvider {
+  public:
+    virtual double onLocalClusterUpdate(const Upstream::Cluster& cluster) PURE;
+  };
+  using ShareMonitorSharedPtr = std::shared_ptr<ShareMonitor>;
+
+private:
+  ShareProviderManager(Event::Dispatcher& main_dispatcher, const Upstream::Cluster& cluster);
+
+  Event::Dispatcher& main_dispatcher_;
+  const Upstream::Cluster& cluster_;
+  Envoy::Common::CallbackHandlePtr handle_;
+  ShareMonitorSharedPtr share_monitor_;
+};
+using ShareProviderManagerSharedPtr = std::shared_ptr<ShareProviderManager>;
+
+class TokenBucketContext {
+public:
+  virtual ~TokenBucketContext() = default;
+
+  virtual uint32_t maxTokens() const PURE;
+  virtual uint32_t remainingTokens() const PURE;
+  virtual absl::optional<int64_t> remainingFillInterval() const PURE;
+};
+
+class RateLimitTokenBucket : public TokenBucketContext {
+public:
+  virtual bool consume(double factor = 1.0) PURE;
+  virtual void onFillTimer(uint64_t refill_counter, double factor = 1.0) PURE;
+  virtual std::chrono::milliseconds fillInterval() const PURE;
+  virtual double fillRate() const PURE;
+};
+using RateLimitTokenBucketSharedPtr = std::shared_ptr<RateLimitTokenBucket>;
+
+class LocalRateLimiterImpl;
+
+// Token bucket that implements based on the periodic timer.
+class TimerTokenBucket : public RateLimitTokenBucket {
+public:
+  TimerTokenBucket(uint32_t max_tokens, uint32_t tokens_per_fill,
+                   std::chrono::milliseconds fill_interval, uint64_t multiplier,
+                   LocalRateLimiterImpl& parent);
+
+  // RateLimitTokenBucket
+  bool consume(double factor) override;
+  void onFillTimer(uint64_t refill_counter, double factor) override;
+  std::chrono::milliseconds fillInterval() const override { return fill_interval_; }
+  double fillRate() const override { return fill_rate_; }
+  uint32_t maxTokens() const override { return max_tokens_; }
+  uint32_t remainingTokens() const override { return tokens_.load(); }
+  absl::optional<int64_t> remainingFillInterval() const override;
+
+  // Descriptor refill interval is a multiple of the timer refill interval.
+  // For example, if the descriptor refill interval is 150ms and the global
+  // refill interval is 50ms, the value is 3. Every 3rd invocation of
+  // the global timer, the descriptor is refilled.
+  const uint64_t multiplier_{};
+  LocalRateLimiterImpl& parent_;
+  std::atomic<uint32_t> tokens_{};
+  std::atomic<MonotonicTime> fill_time_{};
+
+  const uint32_t max_tokens_{};
+  const uint32_t tokens_per_fill_{};
+  const std::chrono::milliseconds fill_interval_{};
+  const double fill_rate_{};
+};
+
+class AtomicTokenBucket : public RateLimitTokenBucket {
+public:
+  AtomicTokenBucket(uint32_t max_tokens, uint32_t tokens_per_fill,
+                    std::chrono::milliseconds fill_interval, TimeSource& time_source);
+
+  // RateLimitTokenBucket
+  bool consume(double factor) override;
+  void onFillTimer(uint64_t, double) override {}
+  std::chrono::milliseconds fillInterval() const override { return {}; }
+  double fillRate() const override { return token_bucket_.fillRate(); }
+  uint32_t maxTokens() const override { return static_cast<uint32_t>(token_bucket_.maxTokens()); }
+  uint32_t remainingTokens() const override {
+    return static_cast<uint32_t>(token_bucket_.remainingTokens());
+  }
+  absl::optional<int64_t> remainingFillInterval() const override { return {}; }
+
+private:
+  AtomicTokenBucketImpl token_bucket_;
+};
+
 class LocalRateLimiterImpl {
 public:
   LocalRateLimiterImpl(
@@ -86,6 +196,7 @@ private:
   std::vector<LocalDescriptorImpl> sorted_descriptors_;
   mutable Thread::ThreadSynchronizer synchronizer_; // Used for testing only.
   const bool always_consume_default_token_bucket_{};
+  const bool no_timer_based_rate_limit_token_bucket_{};
 
   friend class LocalRateLimiterImplTest;
 };
diff --git a/test/common/common/BUILD b/test/common/common/BUILD
index ede00f4bc1..5a2be7a86e 100644
--- test/common/common/BUILD
+++ test/common/common/BUILD
@@ -335,6 +335,7 @@ envoy_cc_test(
     deps = [
         "//source/common/common:token_bucket_impl_lib",
         "//test/test_common:simulated_time_system_lib",
+        "//test/test_common:test_time_lib",
         "//test/test_common:utility_lib",
     ],
 )
diff --git a/test/common/common/token_bucket_impl_test.cc b/test/common/common/token_bucket_impl_test.cc
index 66308cfded..ce20de2cfb 100644
--- test/common/common/token_bucket_impl_test.cc
+++ test/common/common/token_bucket_impl_test.cc
@@ -3,6 +3,7 @@
 #include "source/common/common/token_bucket_impl.h"
 
 #include "test/test_common/simulated_time_system.h"
+#include "test/test_common/test_time.h"
 
 #include "gtest/gtest.h"
 
@@ -126,4 +127,172 @@ TEST_F(TokenBucketImplTest, YearlyMinRefillRate) {
   EXPECT_EQ(1, token_bucket.consume(1, false));
 }
 
+class AtomicTokenBucketImplTest : public testing::Test {
+protected:
+  Event::SimulatedTimeSystem time_system_;
+};
+
+// Verifies TokenBucket initialization.
+TEST_F(AtomicTokenBucketImplTest, Initialization) {
+  AtomicTokenBucketImpl token_bucket{1, time_system_, -1.0};
+
+  EXPECT_EQ(1, token_bucket.fillRate());
+  EXPECT_EQ(1, token_bucket.maxTokens());
+  EXPECT_EQ(1, token_bucket.remainingTokens());
+
+  EXPECT_EQ(1, token_bucket.consume(1, false));
+  EXPECT_EQ(0, token_bucket.consume(1, false));
+  EXPECT_EQ(false, token_bucket.consume());
+}
+
+// Verifies TokenBucket's maximum capacity.
+TEST_F(AtomicTokenBucketImplTest, MaxBucketSize) {
+  AtomicTokenBucketImpl token_bucket{3, time_system_, 1};
+
+  EXPECT_EQ(1, token_bucket.fillRate());
+  EXPECT_EQ(3, token_bucket.maxTokens());
+  EXPECT_EQ(3, token_bucket.remainingTokens());
+
+  EXPECT_EQ(3, token_bucket.consume(3, false));
+  time_system_.setMonotonicTime(std::chrono::seconds(10));
+  EXPECT_EQ(0, token_bucket.consume(4, false));
+  EXPECT_EQ(3, token_bucket.consume(3, false));
+}
+
+// Verifies that TokenBucket can consume tokens.
+TEST_F(AtomicTokenBucketImplTest, Consume) {
+  AtomicTokenBucketImpl token_bucket{10, time_system_, 1};
+
+  EXPECT_EQ(0, token_bucket.consume(20, false));
+  EXPECT_EQ(9, token_bucket.consume(9, false));
+
+  // consume() == consume(1, false)
+  EXPECT_EQ(true, token_bucket.consume());
+
+  time_system_.setMonotonicTime(std::chrono::milliseconds(999));
+  EXPECT_EQ(0, token_bucket.consume(1, false));
+
+  time_system_.setMonotonicTime(std::chrono::milliseconds(5999));
+  EXPECT_EQ(0, token_bucket.consume(6, false));
+
+  time_system_.setMonotonicTime(std::chrono::milliseconds(6000));
+  EXPECT_EQ(6, token_bucket.consume(6, false));
+  EXPECT_EQ(0, token_bucket.consume(1, false));
+}
+
+// Verifies that TokenBucket can refill tokens.
+TEST_F(AtomicTokenBucketImplTest, Refill) {
+  AtomicTokenBucketImpl token_bucket{1, time_system_, 0.5};
+  EXPECT_EQ(1, token_bucket.consume(1, false));
+
+  time_system_.setMonotonicTime(std::chrono::milliseconds(500));
+  EXPECT_EQ(0, token_bucket.consume(1, false));
+  time_system_.setMonotonicTime(std::chrono::milliseconds(1500));
+  EXPECT_EQ(0, token_bucket.consume(1, false));
+  time_system_.setMonotonicTime(std::chrono::milliseconds(2000));
+  EXPECT_EQ(1, token_bucket.consume(1, false));
+}
+
+// Test partial consumption of tokens.
+TEST_F(AtomicTokenBucketImplTest, PartialConsumption) {
+  AtomicTokenBucketImpl token_bucket{16, time_system_, 16};
+  EXPECT_EQ(16, token_bucket.consume(18, true));
+  time_system_.advanceTimeWait(std::chrono::milliseconds(62));
+  EXPECT_EQ(0, token_bucket.consume(1, true));
+  time_system_.advanceTimeWait(std::chrono::milliseconds(1));
+  EXPECT_EQ(1, token_bucket.consume(2, true));
+}
+
+// Validate that a minimal refresh time is 1 year.
+TEST_F(AtomicTokenBucketImplTest, YearlyMinRefillRate) {
+  constexpr uint64_t seconds_per_year = 365 * 24 * 60 * 60;
+  // Set the fill rate to be 2 years.
+  AtomicTokenBucketImpl token_bucket{1, time_system_, 1.0 / (seconds_per_year * 2)};
+
+  // Consume first token.
+  EXPECT_EQ(1, token_bucket.consume(1, false));
+
+  // Less than a year should still have no tokens.
+  time_system_.setMonotonicTime(std::chrono::seconds(seconds_per_year - 1));
+  EXPECT_EQ(0, token_bucket.consume(1, false));
+  time_system_.setMonotonicTime(std::chrono::seconds(seconds_per_year));
+  EXPECT_EQ(1, token_bucket.consume(1, false));
+}
+
+TEST_F(AtomicTokenBucketImplTest, ConsumeNegativeTokens) {
+  AtomicTokenBucketImpl token_bucket{10, time_system_, 1};
+
+  EXPECT_EQ(3, token_bucket.consume([](double) { return 3; }));
+  EXPECT_EQ(7, token_bucket.remainingTokens());
+  EXPECT_EQ(-3, token_bucket.consume([](double) { return -3; }));
+  EXPECT_EQ(10, token_bucket.remainingTokens());
+}
+
+TEST_F(AtomicTokenBucketImplTest, ConsumeSuperLargeTokens) {
+  AtomicTokenBucketImpl token_bucket{10, time_system_, 1};
+
+  EXPECT_EQ(100, token_bucket.consume([](double) { return 100; }));
+  EXPECT_EQ(-90, token_bucket.remainingTokens());
+}
+
+TEST_F(AtomicTokenBucketImplTest, MultipleThreadsConsume) {
+  // Real time source to ensure we will not fall into endless loop.
+  Event::TestRealTimeSystem real_time_source;
+
+  AtomicTokenBucketImpl token_bucket{1200, time_system_, 1.0};
+
+  // Exhaust all tokens.
+  EXPECT_EQ(1200, token_bucket.consume(1200, false));
+  EXPECT_EQ(0, token_bucket.consume(1, false));
+
+  std::vector<std::thread> threads;
+  auto timeout_point = real_time_source.monotonicTime() + std::chrono::seconds(30);
+
+  size_t thread_1_token = 0;
+  threads.push_back(std::thread([&] {
+    while (thread_1_token < 300 && real_time_source.monotonicTime() < timeout_point) {
+      thread_1_token += token_bucket.consume(1, false);
+    }
+  }));
+
+  size_t thread_2_token = 0;
+  threads.push_back(std::thread([&] {
+    while (thread_2_token < 300 && real_time_source.monotonicTime() < timeout_point) {
+      thread_2_token += token_bucket.consume(1, false);
+    }
+  }));
+
+  size_t thread_3_token = 0;
+  threads.push_back(std::thread([&] {
+    while (thread_3_token < 300 && real_time_source.monotonicTime() < timeout_point) {
+      const size_t left = 300 - thread_3_token;
+      thread_3_token += token_bucket.consume(std::min<size_t>(left, 2), true);
+    }
+  }));
+
+  size_t thread_4_token = 0;
+  threads.push_back(std::thread([&] {
+    while (thread_4_token < 300 && real_time_source.monotonicTime() < timeout_point) {
+      const size_t left = 300 - thread_4_token;
+      thread_4_token += token_bucket.consume(std::min<size_t>(left, 3), true);
+    }
+  }));
+
+  // Fill the buckets by changing the time.
+  for (size_t i = 0; i < 200; i++) {
+    time_system_.advanceTimeWait(std::chrono::seconds(1));
+  }
+  for (size_t i = 0; i < 100; i++) {
+    time_system_.advanceTimeWait(std::chrono::seconds(10));
+  }
+
+  for (auto& thread : threads) {
+    thread.join();
+  }
+
+  EXPECT_EQ(1200, thread_1_token + thread_2_token + thread_3_token + thread_4_token);
+
+  EXPECT_EQ(0, token_bucket.consume(1, false));
+}
+
 } // namespace Envoy
diff --git a/test/extensions/filters/common/local_ratelimit/BUILD b/test/extensions/filters/common/local_ratelimit/BUILD
index 96bd5d38a4..001ef96a4b 100644
--- test/extensions/filters/common/local_ratelimit/BUILD
+++ test/extensions/filters/common/local_ratelimit/BUILD
@@ -14,5 +14,9 @@ envoy_cc_test(
     deps = [
         "//source/extensions/filters/common/local_ratelimit:local_ratelimit_lib",
         "//test/mocks/event:event_mocks",
+        "//test/mocks/upstream:cluster_manager_mocks",
+        "//test/mocks/upstream:cluster_priority_set_mocks",
+        "//test/test_common:test_runtime_lib",
+        "//test/test_common:utility_lib",
     ],
 )
diff --git a/test/extensions/filters/common/local_ratelimit/local_ratelimit_test.cc b/test/extensions/filters/common/local_ratelimit/local_ratelimit_test.cc
index 9c1ead992a..d66b0ec714 100644
--- test/extensions/filters/common/local_ratelimit/local_ratelimit_test.cc
+++ test/extensions/filters/common/local_ratelimit/local_ratelimit_test.cc
@@ -1,6 +1,10 @@
 #include "source/extensions/filters/common/local_ratelimit/local_ratelimit_impl.h"
 
 #include "test/mocks/event/mocks.h"
+#include "test/mocks/upstream/cluster_manager.h"
+#include "test/mocks/upstream/cluster_priority_set.h"
+#include "test/test_common/test_runtime.h"
+#include "test/test_common/thread_factory_for_test.h"
 #include "test/test_common/utility.h"
 
 #include "gmock/gmock.h"
@@ -24,7 +28,10 @@ public:
   }
 
   void initialize(const std::chrono::milliseconds fill_interval, const uint32_t max_tokens,
-                  const uint32_t tokens_per_fill) {
+                  const uint32_t tokens_per_fill, ShareProviderSharedPtr share_provider = nullptr) {
+    TestScopedRuntime runtime;
+    runtime.mergeValues(
+        {{"envoy.reloadable_features.no_timer_based_rate_limit_token_bucket", "false"}});
 
     initializeTimer();
 
@@ -32,6 +39,14 @@ public:
         fill_interval, max_tokens, tokens_per_fill, dispatcher_, descriptors_);
   }
 
+  void initializeWithAtomicTokenBucket(const std::chrono::milliseconds fill_interval,
+                                       const uint32_t max_tokens, const uint32_t tokens_per_fill,
+                                       ShareProviderSharedPtr share_provider = nullptr) {
+    rate_limiter_ =
+        std::make_shared<LocalRateLimiterImpl>(fill_interval, max_tokens, tokens_per_fill,
+                                               dispatcher_, descriptors_, true, share_provider);
+  }
+
   Thread::ThreadSynchronizer& synchronizer() { return rate_limiter_->synchronizer_; }
   Envoy::Protobuf::RepeatedPtrField<
       envoy::extensions::common::ratelimit::v3::LocalRateLimitDescriptor>
@@ -213,11 +228,23 @@ public:
   void initializeWithDescriptor(const std::chrono::milliseconds fill_interval,
                                 const uint32_t max_tokens, const uint32_t tokens_per_fill) {
 
+    TestScopedRuntime runtime;
+    runtime.mergeValues(
+        {{"envoy.reloadable_features.no_timer_based_rate_limit_token_bucket", "false"}});
+
     initializeTimer();
 
     rate_limiter_ = std::make_shared<LocalRateLimiterImpl>(
         fill_interval, max_tokens, tokens_per_fill, dispatcher_, descriptors_);
   }
+
+  void initializeWithAtomicTokenBucketDescriptor(const std::chrono::milliseconds fill_interval,
+                                                 const uint32_t max_tokens,
+                                                 const uint32_t tokens_per_fill) {
+    rate_limiter_ = std::make_shared<LocalRateLimiterImpl>(
+        fill_interval, max_tokens, tokens_per_fill, dispatcher_, descriptors_);
+  }
+
   static constexpr absl::string_view single_descriptor_config_yaml = R"(
   entries:
   - key: foo2
@@ -556,6 +583,294 @@ TEST_F(LocalRateLimiterDescriptorImplTest, TokenBucketDifferentDescriptorStatus)
   EXPECT_EQ(rate_limiter_->remainingFillInterval(descriptor_), 3);
 }
 
+// Verify token bucket functionality with a single token.
+TEST_F(LocalRateLimiterImplTest, AtomicTokenBucket) {
+  initializeWithAtomicTokenBucket(std::chrono::milliseconds(200), 1, 1);
+
+  // 1 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+
+  // 0 -> 1 tokens
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(200));
+
+  // 1 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+
+  // 0 -> 1 tokens
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(200));
+
+  // 1 -> 1 tokens
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(200));
+
+  // 1 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+}
+
+// Verify token bucket functionality with max tokens and tokens per fill > 1.
+TEST_F(LocalRateLimiterImplTest, AtomicTokenBucketMultipleTokensPerFill) {
+  initializeWithAtomicTokenBucket(std::chrono::milliseconds(200), 2, 2);
+
+  // 2 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+  EXPECT_TRUE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+
+  // 0 -> 2 tokens
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(200));
+
+  // 2 -> 1 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+
+  // 1 -> 2 tokens
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(200));
+
+  // 2 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+  EXPECT_TRUE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+}
+
+// Verify token bucket functionality with max tokens and tokens per fill > 1 and
+// share provider is used.
+TEST_F(LocalRateLimiterImplTest, AtomicTokenBucketMultipleTokensPerFillWithShareProvider) {
+  auto share_provider = std::make_shared<MockShareProvider>();
+  EXPECT_CALL(*share_provider, getTokensShareFactor())
+      .WillRepeatedly(testing::Invoke([]() -> double { return 0.5; }));
+
+  initializeWithAtomicTokenBucket(std::chrono::milliseconds(200), 2, 2, share_provider);
+
+  // Every request will consume 1 / factor = 2 tokens.
+
+  // The limiter will be initialized with max tokens and will be consumed at once.
+  // 2 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+
+  // 0 -> 2 tokens
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(200));
+
+  // 2 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+
+  // 0 -> 2 tokens
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(200));
+
+  // 2 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+}
+
+// Verify token bucket functionality with max tokens > tokens per fill.
+TEST_F(LocalRateLimiterImplTest, AtomicTokenBucketMaxTokensGreaterThanTokensPerFill) {
+  initializeWithAtomicTokenBucket(std::chrono::milliseconds(200), 2, 1);
+
+  // 2 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+  EXPECT_TRUE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+
+  // 0 -> 1 tokens
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(200));
+
+  // 1 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+}
+
+// Verify token bucket status of max tokens, remaining tokens and remaining fill interval.
+TEST_F(LocalRateLimiterImplTest, AtomicTokenBucketStatus) {
+  initializeWithAtomicTokenBucket(std::chrono::milliseconds(3000), 2, 2);
+
+  // 2 -> 1 tokens
+  auto rate_limit_result = rate_limiter_->requestAllowed(route_descriptors_);
+  EXPECT_TRUE(rate_limit_result.allowed);
+
+  EXPECT_EQ(rate_limit_result.token_bucket_context->maxTokens(), 2);
+  EXPECT_EQ(rate_limit_result.token_bucket_context->remainingTokens(), 1);
+
+  // 1 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+
+  // 0 -> 1 tokens
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(1500));
+
+  // Note that the route descriptors are not changed so we can reuse the same token bucket context.
+  EXPECT_EQ(rate_limit_result.token_bucket_context->maxTokens(), 2);
+  EXPECT_EQ(rate_limit_result.token_bucket_context->remainingTokens(), 1);
+
+  // 1 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+
+  // 0 -> 0 tokens
+  EXPECT_FALSE(rate_limiter_->requestAllowed(route_descriptors_).allowed);
+
+  // Note that the route descriptors are not changed so we can reuse the same token bucket context.
+  EXPECT_EQ(rate_limit_result.token_bucket_context->maxTokens(), 2);
+  EXPECT_EQ(rate_limit_result.token_bucket_context->remainingTokens(), 0);
+
+  // 0 -> 2 tokens
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(3000));
+
+  // Note that the route descriptors are not changed so we can reuse the same token bucket context.
+  EXPECT_EQ(rate_limit_result.token_bucket_context->maxTokens(), 2);
+  EXPECT_EQ(rate_limit_result.token_bucket_context->remainingTokens(), 2);
+}
+
+TEST_F(LocalRateLimiterDescriptorImplTest, AtomicTokenBucketDescriptorBase) {
+  TestUtility::loadFromYaml(fmt::format(single_descriptor_config_yaml, 1, 1, "0.1s"),
+                            *descriptors_.Add());
+  initializeWithAtomicTokenBucketDescriptor(std::chrono::milliseconds(50), 1, 1);
+
+  EXPECT_TRUE(rate_limiter_->requestAllowed(descriptor_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(descriptor_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(descriptor_).allowed);
+}
+
+TEST_F(LocalRateLimiterDescriptorImplTest, AtomicTokenBucketDescriptor) {
+  TestUtility::loadFromYaml(fmt::format(single_descriptor_config_yaml, 1, 1, "0.1s"),
+                            *descriptors_.Add());
+  initializeWithAtomicTokenBucketDescriptor(std::chrono::milliseconds(50), 1, 1);
+
+  // 1 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(descriptor_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(descriptor_).allowed);
+
+  // 0 -> 1 tokens
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(100));
+
+  // 1 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(descriptor_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(descriptor_).allowed);
+
+  // 0 -> 1 tokens
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(100));
+
+  // 1 -> 1 tokens
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(100));
+
+  // 1 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(descriptor_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(descriptor_).allowed);
+}
+
+// Verify token bucket functionality with request per unit > 1.
+TEST_F(LocalRateLimiterDescriptorImplTest, AtomicTokenBucketMultipleTokensPerFillDescriptor) {
+  TestUtility::loadFromYaml(fmt::format(single_descriptor_config_yaml, 2, 2, "0.1s"),
+                            *descriptors_.Add());
+  initializeWithAtomicTokenBucketDescriptor(std::chrono::milliseconds(50), 2, 2);
+
+  // 2 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(descriptor_).allowed);
+  EXPECT_TRUE(rate_limiter_->requestAllowed(descriptor_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(descriptor_).allowed);
+
+  // 0 -> 2 tokens
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(100));
+
+  // 2 -> 1 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(descriptor_).allowed);
+
+  // 1 -> 2 tokens
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(50));
+
+  // 2 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(descriptor_).allowed);
+  EXPECT_TRUE(rate_limiter_->requestAllowed(descriptor_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(descriptor_).allowed);
+}
+
+// Verify token bucket functionality with multiple descriptors.
+TEST_F(LocalRateLimiterDescriptorImplTest,
+       AtomicTokenBucketDifferentDescriptorDifferentRateLimits) {
+  TestUtility::loadFromYaml(multiple_descriptor_config_yaml, *descriptors_.Add());
+  TestUtility::loadFromYaml(fmt::format(single_descriptor_config_yaml, 1, 1, "2s"),
+                            *descriptors_.Add());
+  initializeWithAtomicTokenBucketDescriptor(std::chrono::milliseconds(1000), 3, 3);
+
+  // 1 -> 0 tokens for descriptor_ and descriptor2_
+  EXPECT_TRUE(rate_limiter_->requestAllowed(descriptor2_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(descriptor2_).allowed);
+  EXPECT_TRUE(rate_limiter_->requestAllowed(descriptor_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(descriptor_).allowed);
+
+  // 0 -> 1 tokens for descriptor2_
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(1000));
+
+  // 1 -> 0 tokens for descriptor2_ and 0 only for descriptor_
+  EXPECT_TRUE(rate_limiter_->requestAllowed(descriptor2_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(descriptor2_).allowed);
+  EXPECT_FALSE(rate_limiter_->requestAllowed(descriptor_).allowed);
+}
+
+// Verify token bucket functionality with multiple descriptors sorted.
+TEST_F(LocalRateLimiterDescriptorImplTest,
+       AtomicTokenBucketDifferentDescriptorDifferentRateLimitsSorted) {
+  TestUtility::loadFromYaml(multiple_descriptor_config_yaml, *descriptors_.Add());
+  TestUtility::loadFromYaml(fmt::format(single_descriptor_config_yaml, 2, 2, "1s"),
+                            *descriptors_.Add());
+  initializeWithAtomicTokenBucketDescriptor(std::chrono::milliseconds(50), 3, 3);
+
+  std::vector<RateLimit::LocalDescriptor> descriptors{{{{"hello", "world"}, {"foo", "bar"}}},
+                                                      {{{"foo2", "bar2"}}}};
+
+  // Descriptors are sorted as descriptor2 < descriptor < global
+  // Descriptor2 from 1 -> 0 tokens
+  // Descriptor from 2 -> 1 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(descriptors).allowed);
+  // Request limited by descriptor2 and won't consume tokens from descriptor.
+  // Descriptor2 from 0 -> 0 tokens
+  // Descriptor from 1 -> 1 tokens
+  EXPECT_FALSE(rate_limiter_->requestAllowed(descriptors).allowed);
+  // Descriptor from 1 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(descriptor_).allowed);
+  // Descriptor from 0 -> 0 tokens
+  EXPECT_FALSE(rate_limiter_->requestAllowed(descriptor_).allowed);
+}
+
+// Verify token bucket status of max tokens, remaining tokens and remaining fill interval.
+TEST_F(LocalRateLimiterDescriptorImplTest, AtomicTokenBucketDescriptorStatus) {
+  TestUtility::loadFromYaml(fmt::format(single_descriptor_config_yaml, 2, 2, "3s"),
+                            *descriptors_.Add());
+  initializeWithAtomicTokenBucketDescriptor(std::chrono::milliseconds(1000), 2, 2);
+
+  // 2 -> 1 tokens
+  auto rate_limit_result = rate_limiter_->requestAllowed(descriptor_);
+
+  EXPECT_TRUE(rate_limit_result.allowed);
+
+  // Note that the route descriptors are not changed so we can reuse the same token bucket context.
+  EXPECT_EQ(rate_limit_result.token_bucket_context->maxTokens(), 2);
+  EXPECT_EQ(rate_limit_result.token_bucket_context->remainingTokens(), 1);
+
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(500));
+
+  // 1 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(descriptor_).allowed);
+  // Note that the route descriptors are not changed so we can reuse the same token bucket context.
+  EXPECT_EQ(rate_limit_result.token_bucket_context->maxTokens(), 2);
+  EXPECT_EQ(rate_limit_result.token_bucket_context->remainingTokens(), 0);
+
+  // 0 -> 1 tokens. 1500ms passed and 1 token will be added.
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(1000));
+
+  // 1 -> 0 tokens
+  EXPECT_TRUE(rate_limiter_->requestAllowed(descriptor_).allowed);
+  // Note that the route descriptors are not changed so we can reuse the same token bucket context.
+  EXPECT_EQ(rate_limit_result.token_bucket_context->maxTokens(), 2);
+  EXPECT_EQ(rate_limit_result.token_bucket_context->remainingTokens(), 0);
+
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(3000));
+
+  // 0 -> 2 tokens
+  // Note that the route descriptors are not changed so we can reuse the same token bucket context.
+  EXPECT_EQ(rate_limit_result.token_bucket_context->maxTokens(), 2);
+  EXPECT_EQ(rate_limit_result.token_bucket_context->remainingTokens(), 2);
+}
+
 } // Namespace LocalRateLimit
 } // namespace Common
 } // namespace Filters
diff --git a/test/extensions/filters/http/local_ratelimit/BUILD b/test/extensions/filters/http/local_ratelimit/BUILD
index 5d997f8345..c68155f64e 100644
--- test/extensions/filters/http/local_ratelimit/BUILD
+++ test/extensions/filters/http/local_ratelimit/BUILD
@@ -20,6 +20,9 @@ envoy_extension_cc_test(
         "//test/common/stream_info:test_util",
         "//test/mocks/http:http_mocks",
         "//test/mocks/local_info:local_info_mocks",
+        "//test/mocks/upstream:cluster_manager_mocks",
+        "//test/test_common:test_runtime_lib",
+        "//test/test_common:utility_lib",
         "@envoy_api//envoy/extensions/filters/http/local_ratelimit/v3:pkg_cc_proto",
     ],
 )
diff --git a/test/extensions/filters/http/local_ratelimit/filter_test.cc b/test/extensions/filters/http/local_ratelimit/filter_test.cc
index f5b7e121a1..f222e6418e 100644
--- test/extensions/filters/http/local_ratelimit/filter_test.cc
+++ test/extensions/filters/http/local_ratelimit/filter_test.cc
@@ -4,6 +4,9 @@
 
 #include "test/mocks/http/mocks.h"
 #include "test/mocks/local_info/mocks.h"
+#include "test/mocks/upstream/cluster_manager.h"
+#include "test/test_common/test_runtime.h"
+#include "test/test_common/thread_factory_for_test.h"
 
 #include "gmock/gmock.h"
 #include "gtest/gtest.h"
@@ -293,6 +296,31 @@ TEST_F(FilterTest, RequestRateLimitedXRateLimitHeaders) {
   auto request_headers = Http::TestRequestHeaderMapImpl();
   auto response_headers = Http::TestResponseHeaderMapImpl();
 
+  EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_->decodeHeaders(request_headers, false));
+  EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_->encodeHeaders(response_headers, false));
+  EXPECT_EQ("1", response_headers.get_("x-ratelimit-limit"));
+  EXPECT_EQ("0", response_headers.get_("x-ratelimit-remaining"));
+  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
+            filter_2_->decodeHeaders(request_headers, false));
+  EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_2_->encodeHeaders(response_headers, false));
+  EXPECT_EQ("1", response_headers.get_("x-ratelimit-limit"));
+  EXPECT_EQ("0", response_headers.get_("x-ratelimit-remaining"));
+  EXPECT_EQ(2U, findCounter("test.http_local_rate_limit.enabled"));
+  EXPECT_EQ(1U, findCounter("test.http_local_rate_limit.enforced"));
+  EXPECT_EQ(1U, findCounter("test.http_local_rate_limit.ok"));
+  EXPECT_EQ(1U, findCounter("test.http_local_rate_limit.rate_limited"));
+}
+
+TEST_F(FilterTest, RequestRateLimitedXRateLimitHeadersWithTimerBasedTokenBucket) {
+  TestScopedRuntime runtime;
+  runtime.mergeValues(
+      {{"envoy.reloadable_features.no_timer_based_rate_limit_token_bucket", "false"}});
+
+  setup(fmt::format(config_yaml, "false", "1", "false", "DRAFT_VERSION_03"));
+
+  auto request_headers = Http::TestRequestHeaderMapImpl();
+  auto response_headers = Http::TestResponseHeaderMapImpl();
+
   EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_->decodeHeaders(request_headers, false));
   EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_->encodeHeaders(response_headers, false));
   EXPECT_EQ("1", response_headers.get_("x-ratelimit-limit"));
@@ -310,9 +338,20 @@ TEST_F(FilterTest, RequestRateLimitedXRateLimitHeaders) {
   EXPECT_EQ(1U, findCounter("test.http_local_rate_limit.rate_limited"));
 }
 
+TEST_F(FilterTest, RequestRateLimitedXRateLimitHeadersWithoutRunningDecodeHeaders) {
+  auto response_headers = Http::TestResponseHeaderMapImpl();
+
+  EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_->encodeHeaders(response_headers, false));
+  EXPECT_EQ("", response_headers.get_("x-ratelimit-limit"));
+  EXPECT_EQ("", response_headers.get_("x-ratelimit-remaining"));
+
+  EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_2_->encodeHeaders(response_headers, false));
+  EXPECT_EQ("", response_headers.get_("x-ratelimit-limit"));
+  EXPECT_EQ("", response_headers.get_("x-ratelimit-remaining"));
+}
+
 static constexpr absl::string_view descriptor_config_yaml = R"(
 stat_prefix: test
-token_bucket:
   max_tokens: {}
   tokens_per_fill: 1
   fill_interval: 60s
@@ -662,6 +701,33 @@ TEST_F(DescriptorFilterTest, RouteDescriptorRequestRatelimitedXRateLimitHeaders)
   auto request_headers = Http::TestRequestHeaderMapImpl();
   auto response_headers = Http::TestResponseHeaderMapImpl();
 
+  EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
+            filter_->decodeHeaders(request_headers, false));
+  EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_->encodeHeaders(response_headers, false));
+  EXPECT_EQ("0", response_headers.get_("x-ratelimit-limit"));
+  EXPECT_EQ("0", response_headers.get_("x-ratelimit-remaining"));
+  EXPECT_EQ(1U, findCounter("test.http_local_rate_limit.enabled"));
+  EXPECT_EQ(1U, findCounter("test.http_local_rate_limit.enforced"));
+  EXPECT_EQ(1U, findCounter("test.http_local_rate_limit.rate_limited"));
+}
+
+TEST_F(DescriptorFilterTest,
+       RouteDescriptorRequestRatelimitedXRateLimitHeadersWithTimerTokenBucket) {
+  TestScopedRuntime runtime;
+  runtime.mergeValues(
+      {{"envoy.reloadable_features.no_timer_based_rate_limit_token_bucket", "false"}});
+
+  setUpTest(fmt::format(descriptor_config_yaml, "0", "DRAFT_VERSION_03", "0", "0"));
+
+  EXPECT_CALL(decoder_callbacks_.route_->route_entry_.rate_limit_policy_,
+              getApplicableRateLimit(0));
+
+  EXPECT_CALL(route_rate_limit_, populateLocalDescriptors(_, _, _, _))
+      .WillOnce(testing::SetArgReferee<0>(descriptor_));
+
+  auto request_headers = Http::TestRequestHeaderMapImpl();
+  auto response_headers = Http::TestResponseHeaderMapImpl();
+
   EXPECT_EQ(Http::FilterHeadersStatus::StopIteration,
             filter_->decodeHeaders(request_headers, false));
   EXPECT_EQ(Http::FilterHeadersStatus::Continue, filter_->encodeHeaders(response_headers, false));
diff --git a/test/extensions/filters/listener/local_ratelimit/local_ratelimit_test.cc b/test/extensions/filters/listener/local_ratelimit/local_ratelimit_test.cc
index 2a6999077f..e4fe0a9180 100644
--- test/extensions/filters/listener/local_ratelimit/local_ratelimit_test.cc
+++ test/extensions/filters/listener/local_ratelimit/local_ratelimit_test.cc
@@ -56,14 +56,9 @@ public:
     NiceMock<Network::MockIoHandle> io_handle_;
   };
 
-  uint64_t initialize(const std::string& filter_yaml, bool expect_timer_create = true) {
+  uint64_t initialize(const std::string& filter_yaml) {
     envoy::extensions::filters::listener::local_ratelimit::v3::LocalRateLimit proto_config;
     TestUtility::loadFromYaml(filter_yaml, proto_config);
-    fill_timer_ = new Event::MockTimer(&dispatcher_);
-    if (expect_timer_create) {
-      EXPECT_CALL(*fill_timer_, enableTimer(_, nullptr));
-      EXPECT_CALL(*fill_timer_, disableTimer());
-    }
     config_ = std::make_shared<FilterConfig>(proto_config, dispatcher_, *stats_store_.rootScope(),
                                              runtime_);
     return proto_config.token_bucket().max_tokens();
@@ -72,7 +67,6 @@ public:
   NiceMock<Event::MockDispatcher> dispatcher_;
   Stats::IsolatedStoreImpl stats_store_;
   NiceMock<Runtime::MockLoader> runtime_;
-  Event::MockTimer* fill_timer_{};
   FilterConfigSharedPtr config_;
 };
 
@@ -116,8 +110,7 @@ token_bucket:
                    ->value());
 
   // Refill the bucket.
-  EXPECT_CALL(*fill_timer_, enableTimer(std::chrono::milliseconds(3000), nullptr));
-  fill_timer_->invokeCallback();
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(3000));
 
   // Third socket is allowed after refill.
   ActiveFilter active_filter3(config_);
diff --git a/test/extensions/filters/network/local_ratelimit/local_ratelimit_integration_test.cc b/test/extensions/filters/network/local_ratelimit/local_ratelimit_integration_test.cc
index ffa3c6e348..56132bcdfb 100644
--- test/extensions/filters/network/local_ratelimit/local_ratelimit_integration_test.cc
+++ test/extensions/filters/network/local_ratelimit/local_ratelimit_integration_test.cc
@@ -57,7 +57,7 @@ typed_config:
   token_bucket:
     max_tokens: 1
     # Set fill_interval to effectively infinite so we only get max_tokens to start and never re-fill.
-    fill_interval: 100000s
+    fill_interval: 1000s
 )EOF");
 
   IntegrationTcpClientPtr tcp_client = makeTcpConnection(lookupPort("listener_0"));
@@ -88,7 +88,7 @@ typed_config:
   token_bucket:
     max_tokens: 2
     # Set fill_interval to effectively infinite so we only get max_tokens to start and never re-fill.
-    fill_interval: 100000s
+    fill_interval: 1000s
 )EOF");
 
     // Clone the whole listener, which includes the `share_key`.
diff --git a/test/extensions/filters/network/local_ratelimit/local_ratelimit_test.cc b/test/extensions/filters/network/local_ratelimit/local_ratelimit_test.cc
index b4e43d7065..95dda20d3b 100644
--- test/extensions/filters/network/local_ratelimit/local_ratelimit_test.cc
+++ test/extensions/filters/network/local_ratelimit/local_ratelimit_test.cc
@@ -12,7 +12,6 @@
 #include "gmock/gmock.h"
 #include "gtest/gtest.h"
 
-using testing::_;
 using testing::InSequence;
 using testing::NiceMock;
 using testing::Return;
@@ -26,14 +25,9 @@ class LocalRateLimitTestBase : public testing::Test, public Event::TestUsingSimu
 public:
   LocalRateLimitTestBase() : singleton_manager_(Thread::threadFactoryForTest()) {}
 
-  uint64_t initialize(const std::string& filter_yaml, bool expect_timer_create = true) {
+  uint64_t initialize(const std::string& filter_yaml) {
     envoy::extensions::filters::network::local_ratelimit::v3::LocalRateLimit proto_config;
     TestUtility::loadFromYamlAndValidate(filter_yaml, proto_config);
-    fill_timer_ = new Event::MockTimer(&dispatcher_);
-    if (expect_timer_create) {
-      EXPECT_CALL(*fill_timer_, enableTimer(_, nullptr));
-      EXPECT_CALL(*fill_timer_, disableTimer());
-    }
     config_ = std::make_shared<Config>(proto_config, dispatcher_, *stats_store_.rootScope(),
                                        runtime_, singleton_manager_);
     return proto_config.token_bucket().max_tokens();
@@ -43,7 +37,6 @@ public:
   Stats::IsolatedStoreImpl stats_store_;
   NiceMock<Runtime::MockLoader> runtime_;
   Singleton::ManagerImpl singleton_manager_;
-  Event::MockTimer* fill_timer_{};
   ConfigSharedPtr config_;
 };
 
@@ -102,8 +95,7 @@ token_bucket:
                    ->value());
 
   // Refill the bucket.
-  EXPECT_CALL(*fill_timer_, enableTimer(std::chrono::milliseconds(200), nullptr));
-  fill_timer_->invokeCallback();
+  dispatcher_.globalTimeSystem().advanceTimeWait(std::chrono::milliseconds(200));
 
   // Third connection is OK.
   ActiveFilter active_filter3(config_);
@@ -145,11 +137,6 @@ public:
     envoy::extensions::filters::network::local_ratelimit::v3::LocalRateLimit proto_config;
     TestUtility::loadFromYamlAndValidate(filter_yaml2, proto_config);
     const uint64_t config2_tokens = proto_config.token_bucket().max_tokens();
-    if (!expect_sharing) {
-      auto timer = new Event::MockTimer(&dispatcher_);
-      EXPECT_CALL(*timer, enableTimer(_, nullptr));
-      EXPECT_CALL(*timer, disableTimer());
-    }
     config2_ = std::make_shared<Config>(proto_config, dispatcher_, *stats_store_.rootScope(),
                                         runtime_, singleton_manager_);
 
